# glue-streaming-nested-data


This repo contains sample code that walks through how to build a glue streaming job that consumes JSON from two Amazon Kinesis Data Streams, Flattens nested JSON and joins the two streams. 
The example demonstrates how to use watermarks to manage late arriving data.

TODO: Add link to blog when posted

## Blog Description

An online eCommerce website has a Machine Learning team that has deployed a services for predict the next best action after a customer makes a purchase. An analytics team has recently been tasked with consuming and joining the outputs of this machine learning service to deliver real time product insights to third partner sellers and partners. 

Each purchase made on the eCommerce website is push to a Kinesis data stream and the down stream machine learning service. Once the machine learning service has processed the purchase, the results are pushed to a separate Kinesis data stream. The purchase record is a nested JSON objects.

To solve this problem, we show you how build a server-less streaming job that joins these two data sources together with AWS Glue Streaming via a stream-stream join (https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins). In this stream job want to de-normalize all records and flattened data to be  used with other analytical services. The following diagram illustrates our solutionâ€™s architecture.
streaming_blog.

![Alt text](docs/images/nested_joins_diagrams_architecture.jpg?raw=true "Title")


In this post, we use synthetic data generated by a Python script. The Python script writes the data to the two Amazon Kinesis Data Streams. Each of these data streams have a Glue Data Catalog table with the nested JSON structure defined. Next, an AWS Glue Streaming job reads the records from each data stream and joins them. With the Glue Streaming job we use native PySpark functions to flatten the nested JSON and join the streaming DataFrames. We also use Watermarks to manage the late arriving data. Finally, we write the data to an Amazon S3 sink.

We will build this solution with the following steps: 

* Consume and flatten nested JSON streamed from Kinesis Data Stream
* Manage late arriving data with watermarks
* Perform joins with data streams
* Write to Sink (S3)

## Deployment
Follow the steps to running the sample code in this blog post: [TODO-Add link to blog when posted]

## Author
Lee McDonald

## License
This library is licensed under the MIT-0 License. See the LICENSE file.
