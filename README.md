# Consuming, flattening and joining multiple JSON data sources with AWS Glue Streaming
This repo contains code that demonstrates how to leverage AWS Glue streaming capabilities to process unbounded datasets, which arrive in the form of nested JSON key-value structures, 
from multiple data producers. For this solution, we will use Amazon Kinesis as a message bus, AWS Glue to process the data and Amazon S3 to store the results

## Solution Overview
In this solution, we show you how to build a serverless streaming job that joins two data sources together with AWS Glue Streaming via a [stream-stream join](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins). 
In this stream job want to de-normalize all records and flattened data, then write it to S3. The following diagram illustrates our solutionâ€™s architecture.

![Alt text](docs/images/nested_joins_diagrams_architecture.jpg?raw=true "Title")

In this solution, we use synthetic data generated by a Python script. The Python script writes the data to the two Amazon Kinesis Data Streams. 
Each of these data streams have a Glue Data Catalog table with the nested JSON structure defined. 
Next, an AWS Glue Streaming job reads the records from each data stream and joins them. 
With the Glue Streaming job we use native PySpark functions to flatten the nested JSON and join the streaming DataFrames. 
We also use Watermarks to manage the late arriving data. Finally, we write the data to an Amazon S3 sink.

### Deploy the CloudFormation Script

To get started, navigate to the AWS CloudFormation console. Create a CloudFormation Stack with the template in the [infra folder](infra/Glue_Streaming_Joining_Nested_Data_cf.yaml).

For more details on how to do this, please see the [CloudFormation documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html) describing how to create a stack in the console.

### Upload the streaming job to S3
Upload the [script](src/streaming_join_job.py) to your new s3 bucket s3://<YOUR-BUCKET-NAME>/code/.

### Configure the Python virtual environment
This project is set up like a standard Python project. 
The initialization process also creates a virtualenv within this project, 
stored under the .venv directory. 
To create the virtualenv it assumes that there is a python3 (or python for Windows) 
executable in your path with access to the venv package. 
If for any reason the automatic creation of the virtualenv fails, you can create the virtualenv manually.
 
```
$ python3 -m venv .venv
```

The next step is to activate your virtualenv.

```
$ source .venv/bin/activate
```
If you are a Windows platform, you would activate the virtualenv like this:
```
% .venv\Scripts\activate.bat
```
Once the virtualenv is activated, you can install the required dependencies.

```
(.venv) $ pip install -r requirements.txt
```

### Run the data generation script
The solution comes with a Python script to generate the two data sources. 
Make sure you have [set up the boto3 credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) correctly. Then in the terminal, navigate to the data-gen folder of the code repo and run the script like below:

```
# Script for generating the data
python3 data_genrator.py --purchase_stream_name nestedPurchaseStream --recommender_stream_name recommenderStream --region ap-southeast-2
```

## License
This library is licensed under the MIT-0 License. See the LICENSE file.
